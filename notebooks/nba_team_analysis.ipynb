{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.stats import rankdata\n",
    "from explore import compute_chatterjee_corr_df, compute_chatterjee_corr_np, split_dataframe_k_folds\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\meeta\\OneDrive\\Documents\\py_projects\\soccer_simple_reg\\data\\raw/NBA_Dataset_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(\n",
    "    columns={\n",
    "        \"Points_Scored\": \"Points\",\n",
    "        \"Weightlifting_Sessions_Average\": \"WL\",\n",
    "        \"Yoga_Sessions_Average\": \"Yoga\",\n",
    "        \"Laps_Run_Per_Practice_Average\": \"Laps\",\n",
    "        \"Water_Intake\": \"WI\",\n",
    "        \"Players_Absent_For_Sessions\": \"PAFS\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df[\"Points\"], kde=True, stat=\"density\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"Points\"], bins=15, color=\"skyblue\", edgecolor=\"black\", density=True)\n",
    "plt.title('Distribution of Points')\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Frequency')\n",
    "sns.kdeplot(data=df[\"Points\"], color=\"red\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df[\"Points\"], color=\"skyblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=df[\"Points\"], color=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_violin_plots(df, x, y):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Get the y-axis limits from the data\n",
    "    y_min = df[y].min() - 10\n",
    "    y_max = df[y].max() + 10\n",
    "    \n",
    "    fig.suptitle(f\"Violin and box plots for variable {y} against {x}\")\n",
    "    \n",
    "    # Create plots with consistent y-axis limits\n",
    "    sns.boxplot(data=df[y], ax=ax[0])\n",
    "    ax[0].set_ylim(y_min, y_max)\n",
    "    ax[0].tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    sns.boxplot(x=x, y=y, data=df, ax=ax[1], hue=x, palette=\"Set2\")\n",
    "    ax[1].set_ylim(y_min, y_max)\n",
    "    ax[1].tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    sns.violinplot(x=x, y=y, data=df, ax=ax[2], hue=x, palette=\"Set2\")\n",
    "    ax[2].set_ylim(y_min, y_max)\n",
    "    ax[2].tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col == \"Team\":\n",
    "        continue\n",
    "    plot_box_violin_plots(df, x=\"Team\", y=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col == \"Team\":\n",
    "        continue\n",
    "    outliers = find_outliers(df, col)\n",
    "    print(f\"Outliers for column {col}:\")\n",
    "    print(outliers)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop(index=[142, 143, 144])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc[df_clean.index == 8, \"WL\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_violin_plots(df_clean, x=\"Team\", y=\"WL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_simple = df_clean.copy()\n",
    "df_clean_iterative = df_clean.copy()\n",
    "df_clean_knn = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().mean() # fraction of null values in each column - same as df_clean.isna().sum() / df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = pd.DataFrame(df_clean.isna().sum() / df_clean.shape[0]).rename(columns={0: \"Null_Proportion\"})\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts.plot(kind=\"bar\", color=\"skyblue\", title=\"Proportion of Missing Values in Each Column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape, df_clean.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_clean[\"WL\"], kde=True, stat=\"density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_clean[\"WL\"].fillna(df_clean[\"WL\"].mean()), kde=True, stat=\"density\") # naive approach using column mean\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_clean[\"WL\"].fillna(df_clean[\"WL\"].median()), kde=True, stat=\"density\") # naive approach using column median\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_clean.fillna(df_clean.loc[:, [\"Team\", \"WL\", \"Yoga\", \"Laps\", \"WI\", \"PAFS\"]].groupby(\"Team\").transform(\"mean\"))[\"WL\"], kde=True, stat=\"density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.fillna(df_clean.loc[:, [\"Team\", \"WL\", \"Yoga\", \"Laps\", \"WI\", \"PAFS\"]].groupby(\"Team\").transform(\"mean\")) # naive approach but more refined because we are using group means (aggregates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Sklearn imputers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"WL\", \"Yoga\", \"Laps\", \"WI\", \"PAFS\"]\n",
    "simple_imputer = SimpleImputer(strategy=\"mean\")\n",
    "df_clean_simple.loc[:, features] = simple_imputer.fit_transform(df_clean_simple.loc[:, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_imputer = IterativeImputer(max_iter=10)\n",
    "df_clean_iterative.loc[:, features] = iterative_imputer.fit_transform(df_clean_iterative.loc[:, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_clean_knn.loc[:, features] = knn_imputer.fit_transform(df_clean_knn.loc[:, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_simple.loc[df_clean.loc[df_clean[\"WL\"].isna()].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_iterative.loc[df_clean.loc[df_clean[\"WL\"].isna()].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_knn.loc[df_clean.loc[df_clean[\"WL\"].isna()].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_iterative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featrues = [\"WL\", \"Yoga\", \"Laps\", \"WI\", \"PAFS\"]\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "sns.histplot(df_clean_iterative[\"WL\"], kde=True, stat=\"density\", ax=ax[0, 0])\n",
    "ax[0, 0].set_title(\"WL - Iterative Imputer\")\n",
    "\n",
    "sns.histplot(df_clean_iterative[\"Yoga\"], kde=True, stat=\"density\", ax=ax[0, 1])\n",
    "ax[0, 1].set_title(\"Yoga - Iterative Imputer\")\n",
    "\n",
    "sns.histplot(df_clean_iterative[\"Laps\"], kde=True, stat=\"density\", ax=ax[0, 2])\n",
    "ax[0, 2].set_title(\"Laps - Iterative Imputer\")\n",
    "\n",
    "sns.histplot(df_clean_iterative[\"WI\"], kde=True, stat=\"density\", ax=ax[1, 0])\n",
    "ax[1, 0].set_title(\"WI - Iterative Imputer\")\n",
    "\n",
    "sns.histplot(df_clean_iterative[\"PAFS\"], kde=True, stat=\"density\", ax=ax[1, 1])\n",
    "ax[1, 1].set_title(\"PAFS - Iterative Imputer\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring different values of bandwidth for kernel density estimation (lower values capture too much noise from individual data points and higher values end up over-smoothening sampled dataset thereby losing information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(16, 8))\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 0.25}, ax=ax[0, 0])\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 0.5}, ax=ax[0, 1])\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 0.75}, ax=ax[0, 2])\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 0.95}, ax=ax[1, 0])\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 1}, ax=ax[1, 1])\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 1.25}, ax=ax[1, 2])\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 1.5}, ax=ax[2, 0])\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 1.75}, ax=ax[2, 1])\n",
    "sns.histplot(df_clean_knn[\"WL\"], kde=True, stat=\"density\", kde_kws={\"bw_adjust\": 2}, ax=ax[2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_iterative.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df[\"Team\"].unique():\n",
    "    print(\"--------------------\")\n",
    "    print(f\"Correlation matrix for team {i}\")\n",
    "    display(df_clean_iterative.loc[df_clean_iterative[\"Team\"] == i].corr(numeric_only=True))\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_clean_iterative, kind=\"scatter\", hue=\"Team\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "sns.heatmap(df_clean_iterative.corr(numeric_only=True), annot=True, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of Chatterjee Correlation Coefficient which verifies non-linear dependence (function) between two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_df(df_clean_iterative, x=\"WL\", y=\"Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_np(df_clean_iterative[\"WL\"].values, df_clean_iterative[\"Points\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_df(df_clean_iterative, x=\"WI\", y=\"Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_np(df_clean_iterative[\"WI\"].values, df_clean_iterative[\"Points\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_df(df_clean_iterative, x=\"Laps\", y=\"Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_np(df_clean_iterative[\"Laps\"].values, df_clean_iterative[\"Points\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_df(df_clean_iterative, x=\"PAFS\", y=\"Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_np(df_clean_iterative[\"PAFS\"].values, df_clean_iterative[\"Points\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_df(df_clean_iterative, x=\"Yoga\", y=\"Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chatterjee_corr_np(df_clean_iterative[\"Yoga\"].values, df_clean_iterative[\"Points\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df = pd.get_dummies(data=df_clean, columns=[\"Team\"], dtype=\"int\", drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_hot_df.drop(columns=[\"Points\"])\n",
    "y = one_hot_df[\"Points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = split_dataframe_k_folds(df=one_hot_df, k=10)\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "for deg in range(1, 10):\n",
    "    train_deg_mse = []\n",
    "    test_deg_mse = []\n",
    "    for df_k in folds:\n",
    "        # create a dataframe unioning all the folds except the current one\n",
    "        df_train = pd.concat([df for df in folds if not df.equals(df_k)])\n",
    "        X_train = df_train.drop(columns=[\"Points\"])\n",
    "        y_train = df_train[\"Points\"]\n",
    "        X_test = df_k.drop(columns=[\"Points\"])\n",
    "        y_test = df_k[\"Points\"]\n",
    "\n",
    "        features = X_train.columns\n",
    "        iterative_imputer = IterativeImputer(max_iter=10)\n",
    "        X_train.loc[:, features] = iterative_imputer.fit_transform(X_train)\n",
    "        X_test.loc[:, features] = iterative_imputer.transform(X_test)\n",
    "\n",
    "        poly_converter = PolynomialFeatures(degree=deg, include_bias=False)\n",
    "        X_train_poly = poly_converter.fit_transform(X_train)\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_poly, y_train)\n",
    "        y_pred = model.predict(poly_converter.transform(X_test))\n",
    "        \n",
    "        train_deg_mse.append(mean_squared_error(y_train, model.predict(X_train_poly)))\n",
    "        test_deg_mse.append(mean_squared_error(y_test, y_pred))\n",
    "    train_mse.append(\n",
    "        (deg, train_deg_mse, np.mean(train_deg_mse))\n",
    "    )\n",
    "    test_mse.append(\n",
    "        (deg, test_deg_mse, np.mean(test_deg_mse))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns\n",
    "iterative_imputer = IterativeImputer(max_iter=10)\n",
    "X_train.loc[:, features] = iterative_imputer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS with intercept using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "lr1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS without intercept using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = sm.OLS(y_train, X_train).fit()\n",
    "lr2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fit with intercept using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.loc[:, features] = iterative_imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "df[\"Points\"].hist(bins=15, color=\"skyblue\", edgecolor=\"black\", density=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Points\"].mean(), df[\"Points\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, lr1.predict(sm.add_constant(X_test))) # MAE for model with predictions using statsmodels  api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_test_pred) # MAE for model with predictions using sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(y_test - y_test_pred)) / y_test.shape[0] # MAE for model using numpy manual computations and predictions from sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, lr1.predict(sm.add_constant(X_test))) # MSE for model with predictions using statsmodels api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_test_pred) # MSE for model with predictions using sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((y_test - y_test_pred) ** 2) / y_test.shape[0] # mean squared error for model using numpy manual computations and predictions sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y_test, lr1.predict(sm.add_constant(X_test))) # RMSE for model with predictions using statsmodels api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y_test, y_test_pred) # RMSE for model with predictions using sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((y_test - y_test_pred) ** 2) / y_test.shape[0]) # root mean squared error for model using numpy manual computations and predictions sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y_test, y_test_pred) / np.mean(df[\"Points\"]) # relative root mean squared error (relative to the mean of the target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y_test, lr1.predict(sm.add_constant(X_test))) / np.mean(df[\"Points\"]) # relative root mean squared error for model with predictions using statsmodels api (relative to the mean of the target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1.rsquared, lr1.rsquared_adj, r2_score(y_train, lr1.predict(sm.add_constant(X_train))), r2_score(y_test, lr1.predict(sm.add_constant(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial of degree 2 using numpy polyfit as well as statsmodels output summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_converter2 = PolynomialFeatures(degree=2, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_converter2.fit(X_train[[\"WL\"]])\n",
    "features = polynomial_converter2.get_feature_names_out([\"WL\"])\n",
    "X_train_poly2 = pd.DataFrame(data=polynomial_converter2.transform(X_train[[\"WL\"]]), columns=features, index=X_train.index)\n",
    "X_train_poly2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(y_train, sm.add_constant(X_train_poly2)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.polyfit(X_train[\"WL\"], y_train, deg=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial of degree 3 using numpy polyfit as well as statsmodels output summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_converter3 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "polynomial_converter3.fit(X_train[[\"WL\"]])\n",
    "features = polynomial_converter3.get_feature_names_out([\"WL\"])\n",
    "X_train_poly3 = pd.DataFrame(data=polynomial_converter3.transform(X_train[[\"WL\"]]), columns=features, index=X_train.index)\n",
    "X_train_poly3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(y_train, sm.add_constant(X_train_poly3)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.polyfit(deg=3, x=X_train[\"WL\"], y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_poly = np.poly1d(np.polyfit(X_train[\"WL\"], y_train, deg=1))\n",
    "quadratic_poly = np.poly1d(np.polyfit(X_train[\"WL\"], y_train, deg=2))\n",
    "cubic_poly = np.poly1d(np.polyfit(X_train[\"WL\"], y_train, deg=3))\n",
    "quartic_poly = np.poly1d(np.polyfit(X_train[\"WL\"], y_train, deg=4))\n",
    "fifth_poly = np.poly1d(np.polyfit(X_train[\"WL\"], y_train, deg=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.linspace(X_train[\"WL\"].min()-10, X_train[\"WL\"].max()+10, X_train[\"WL\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[\"WL\"], y_train, color=\"skyblue\", label=\"Training Data\")\n",
    "plt.plot(values, linear_poly(values), color=\"red\", label=\"Linear Fit\")\n",
    "plt.plot(values, quadratic_poly(values), color=\"green\", label=\"Quadratic Fit\")\n",
    "plt.plot(values, cubic_poly(values), color=\"orange\", label=\"Cubic Fit\")\n",
    "plt.plot(values, quartic_poly(values), color=\"purple\", label=\"Quartic Fit\")\n",
    "plt.plot(values, fifth_poly(values), color=\"black\", label=\"Fifth Degree Fit\")\n",
    "\n",
    "plt.ylim(y_train.min() - 10, y_train.max() + 50)\n",
    "plt.xlim(X_train[\"WL\"].min() - 10, X_train[\"WL\"].max() + 10)\n",
    "plt.xlabel(\"WL\")\n",
    "plt.ylabel(\"Points\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial features over all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_converter = PolynomialFeatures(degree=2, include_bias=False)\n",
    "polynomial_converter.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = polynomial_converter.get_feature_names_out(input_features=X_train.columns)\n",
    "features = [feat.replace(\" \", \"_\") for feat in features]\n",
    "X_train_poly = pd.DataFrame(data=polynomial_converter.fit_transform(X_train), columns=features, index=X_train.index)\n",
    "X_train_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(pd.DataFrame(data=polynomial_converter.transform(X_test), columns=features, index=X_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_poly = sm.OLS(y_train, sm.add_constant(X_train_poly)).fit()\n",
    "lr_poly.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_poly.predict(sm.add_constant(polynomial_converter.transform(X_test)))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y_test, y_pred) # root mean squared error for polynomial regression model using statsmodels api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y_test, y_test_pred) # root mean squared error for polynomial regression model using sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred) # R^2 score for polynomial regression model using statsmodels api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_test_pred) # R^2 score for polynomial regression model using sklearn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".soccer_simple_reg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
